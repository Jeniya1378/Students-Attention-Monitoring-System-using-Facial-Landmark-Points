In this project, we developed a system that combines transfer learning with deep neural network techniques to measure attention levels. The project utilizes the DAiSEE dataset, consisting of video snippets capturing user activities such as boredom, confusion, engagement, and frustration. Preprocessing involves extracting frames from the videos and applying MediaPipe's FaceMesh detection to obtain facial mesh images. The key feature of this system is Transfer learning which exploits the use of Xception architecture for image classification. The trained model is evaluated on a validation split from the dataset, yielding an accuracy of 65.3% for multi-label classification. We explored another baseline model of a fully connected neural network (FCN) that yields an accuracy of 63.3% over two classes: engaged and not engaged. The project provides insights into monitoring student engagement and proposes future work to address dataset imbalance and explore binary classification approaches.
